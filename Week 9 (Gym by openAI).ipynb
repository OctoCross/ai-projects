{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym by OpenAI\n",
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms. <br>\n",
    "It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. <br>\n",
    "\n",
    "The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. <br>\n",
    "These environments have a shared interface, allowing you to write general algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "To get started, you’ll need to have Python 3.5+ installed. Simply install gym using pip: <br>\n",
    "Please take note, this step works well for Mac and Linux. <br>\n",
    "If you are using Windows, you will need to do some workaround, using this link: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30 <br>\n",
    "\n",
    "Please remember, you will need to install the following libraries: <br>\n",
    "1. pystan\n",
    "2. swig\n",
    "3. Box2D\n",
    "\n",
    "You may need to install other libraries depending on the environments that you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install conda-forge::gymnasium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environments\n",
    "We will be looking at a few environments:\n",
    "1. CartPole-v1\n",
    "2. MountainCar-v0\n",
    "3. BipedalWalker-v3\n",
    "4. LunarLander-v2\n",
    "5. CarRacing-v0\n",
    "6. Pendulum-v0\n",
    "7. Acrobot-v1\n",
    "8. Taxi-v3\n",
    "9. Copy-v0\n",
    "\n",
    "We will look at the problems posed. <br>\n",
    "As this is an introductory course, we will not go in-depth of the solutions. <br>\n",
    "You are encouraged to find the answers online by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cart Pole\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. <br>\n",
    "The system is controlled by applying a force of +1 or -1 to the cart. <br>\n",
    "The pendulum starts upright, and the goal is to prevent it from falling over. <br>\n",
    "A reward of +1 is provided for every timestep that the pole remains upright. <br>\n",
    "The episode ends when the pole is more than 15 degrees from vertical or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random-action demo using the maintained Gymnasium package\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment with a display window\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Start a new episode\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# Run for 200 time steps using random actions\n",
    "for _ in range(200):\n",
    "    # Sample a random action from the action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Take a step in the environment\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Episode ends if either condition is True\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "# Always close the environment to release resources\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode return (heuristic): 200.0\n"
     ]
    }
   ],
   "source": [
    "# CartPole: simple heuristic \"solution\" (better than random)\n",
    "# Uses Gymnasium API and proper render_mode\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Make an on-screen environment. Use render_mode=\"rgb_array\" if you're headless.\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "total_reward = 0.0\n",
    "\n",
    "for _ in range(200):\n",
    "    # Observation format: [cart_x, cart_v, pole_angle, pole_ang_vel]\n",
    "    cart_x, cart_v, theta, theta_dot = obs\n",
    "\n",
    "    # Heuristic controller:\n",
    "    # push right if pole leans right (theta > 0), else push left.\n",
    "    # A tiny PD-ish nudge using angular velocity helps stability:\n",
    "    action = int(theta + 0.1 * theta_dot > 0)   # 0 = left, 1 = right\n",
    "\n",
    "    # Take a step\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    # End episode if done or time limit reached\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(f\"Episode return (heuristic): {total_reward:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mountain Car\n",
    "A car is on a one-dimensional track, positioned between two \"mountains\". <br>\n",
    "The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. <br>\n",
    "Therefore, the only way to succeed is to drive back and forth to build up momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MountainCar: random action demo (Gymnasium version)\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment with rendering\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# Reset to start a new episode\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# Run for 5100 steps with random actions\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()        # choose random action (0, 1, or 2)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()               # restart if the episode ends\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode return (heuristic): -101.0\n"
     ]
    }
   ],
   "source": [
    "# MountainCar: simple heuristic demo using Gymnasium\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the environment (set render_mode=\"rgb_array\" if no display)\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "total_reward = 0.0\n",
    "\n",
    "for t in range(5100):\n",
    "    # Observation: [position, velocity]\n",
    "    position, velocity = obs\n",
    "\n",
    "    # Heuristic policy:\n",
    "    # If the car is moving right, keep accelerating right (action=2)\n",
    "    # If it's moving left, accelerate left (action=0)\n",
    "    # This builds momentum to reach the flag at the top.\n",
    "    action = 2 if velocity > 0 else 0\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(f\"Episode return (heuristic): {total_reward:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bipedal\n",
    "Reward is given for moving forward, total 300+ points up to the far end. <br>\n",
    "If the robot falls, it gets -100. <br>\n",
    "Applying motor torque costs a small amount of points, more optimal agent will get better score. <br>\n",
    "State consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. <br>\n",
    "There's no coordinates in the state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BipedalWalker: random action demo using Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment (requires gymnasium[box2d] and pygame)\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")\n",
    "\n",
    "# Reset to start a new episode\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# Run for 1000 steps with random continuous actions\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # sample random torque values (4D)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Restart if the episode ends\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n",
      "C:\\Users\\kchri\\miniconda3\\envs\\reinforcement\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward with smoothing: -116.8\n",
      "Episode reward with smoothing: -120.3\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "alpha = 0.85  # I did some experimenting with this, from 0.8 to 0.95. This one worked best IMO.\n",
    "prev_action = np.zeros(env.action_space.shape, dtype=np.float32)\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "ep_reward = 0.0\n",
    "for t in range(1000):\n",
    "    raw = env.action_space.sample()  \n",
    "    action = alpha * prev_action + (1 - alpha) * raw\n",
    "    action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    prev_action = action\n",
    "    ep_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Episode reward with smoothing: {ep_reward:.1f}\")\n",
    "        obs, info = env.reset()\n",
    "        prev_action[:] = 0\n",
    "        ep_reward = 0.0\n",
    "\n",
    "\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lunar Lander\n",
    "Landing pad is always at coordinates (0,0). <br>\n",
    "Coordinates are the first two numbers in the state vector. <br>\n",
    "Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. <br>\n",
    "If the lander moves away from the landing pad it loses reward back. <br>\n",
    "The episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. <br>\n",
    "Each leg ground contact is +10. The Firing main engine is -0.3 points each frame. Solved is 200 points. <br>\n",
    "Landing outside the landing pad is possible. <br>\n",
    "Fuel is infinite, so an agent can learn to fly and then land on its first attempt. <br>\n",
    "Four discrete actions are available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LunarLander: random action demo using Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment (requires gymnasium[box2d] and pygame)\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# Reset to start a new episode\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# Run for 500 steps with random actions\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()            # 0: do nothing, 1: left, 2: main, 3: right\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:                   # restart if the episode ends\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward (cost-aware sampling): -117.6\n",
      "Episode reward (cost-aware sampling): -156.2\n",
      "Episode reward (cost-aware sampling): -130.6\n",
      "Episode reward (cost-aware sampling): -151.8\n",
      "Episode reward (cost-aware sampling): -123.4\n",
      "Episode reward (cost-aware sampling): -156.6\n",
      "Episode reward (cost-aware sampling): -151.2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()            \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:                   \n",
    "        obs, info = env.reset()\n",
    "\n",
    "A = np.array([0, 1, 2, 3])                       \n",
    "P = np.array([0.45, 0.25, 0.05, 0.25], dtype=float)  \n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "ep_reward = 0.0\n",
    "for t in range(500):\n",
    "    action = int(np.random.choice(A, p=P))       \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    ep_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Episode reward (cost-aware sampling): {ep_reward:.1f}\")\n",
    "        obs, info = env.reset()\n",
    "        ep_reward = 0.0\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Car Racing\n",
    "Easiest continuous control task to learn from pixels, a top-down racing environment.<br> \n",
    "Discreet control is reasonable in this environment as well, on/off discretisation is fine. <br>\n",
    "State consists of 96x96 pixels. <br>\n",
    "Reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. <br>\n",
    "For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. <br>\n",
    "Episode finishes when all tiles are visited. Some indicators shown at the bottom of the window and the state RGB buffer. <br>\n",
    "From left to right: true speed, four ABS sensors, steering wheel position, gyroscope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CarRacing: random action demo using Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment (requires gymnasium[box2d] and pygame)\n",
    "env = gym.make(\"CarRacing-v3\", render_mode=\"human\")\n",
    "\n",
    "# Reset to start a new episode\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# Run for 2000 steps with random continuous actions\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()  # shape (3,) = [steering, gas, brake]\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward (persistent actions): -37.3\n",
      "Episode reward (persistent actions): -25.3\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CarRacing-v3\", render_mode=\"human\")\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "hold_frames = 5          \n",
    "frame_count = 0\n",
    "current_action = env.action_space.sample()\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "ep_reward = 0.0\n",
    "for t in range(2000):\n",
    "   \n",
    "    if frame_count % hold_frames == 0:\n",
    "        current_action = env.action_space.sample()\n",
    "    frame_count += 1\n",
    "\n",
    "    \n",
    "    if t > 0:\n",
    "        current_action = 0.8 * current_action + 0.2 * prev_action\n",
    "    prev_action = current_action\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(current_action)\n",
    "    ep_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Episode reward (persistent actions): {ep_reward:.1f}\")\n",
    "        obs, info = env.reset()\n",
    "        frame_count, ep_reward = 0, 0.0\n",
    "\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()  \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pendulum\n",
    "The inverted pendulum swing-up problem is a classic problem in the control literature. <br>\n",
    "In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendulum: random action demo using Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment (requires gymnasium[classic-control] and pygame)\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "# Reset to start a new episode\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# Run for 1000 steps with random continuous actions\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()      # one continuous torque value in [-2, 2]\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward (PD control): -956.3\n",
      "Episode reward (PD control): -964.0\n",
      "Episode reward (PD control): -1381.2\n",
      "Episode reward (PD control): -1001.4\n",
      "Episode reward (PD control): -930.6\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()      \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "def angle_and_rate(obs):\n",
    "    c, s, thdot = obs\n",
    "    return np.arctan2(s, c), thdot  \n",
    "\n",
    "Kp, Kd = 2.0, 0.5        \n",
    "noise_std = 0.05         \n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "ep_reward = 0.0\n",
    "for t in range(1000):\n",
    "    theta, theta_dot = angle_and_rate(obs)\n",
    "    u = -(Kp * theta + Kd * theta_dot) + np.random.randn() * noise_std\n",
    "    action = np.array([np.clip(u, env.action_space.low[0], env.action_space.high[0])], dtype=np.float32)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    ep_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Episode reward (PD control): {ep_reward:.1f}\")\n",
    "        obs, info, ep_reward = env.reset(),  env.reset()[1], 0.0\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Acrobot\n",
    "The acrobot system includes two joints and two links, where the joint between the two links is actuated. <br>\n",
    "Initially, the links are hanging downwards, and the goal is to swing the end of the lower link up to a given height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acrobot: random action demo using Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment (requires gymnasium[classic-control] and pygame)\n",
    "env = gym.make(\"Acrobot-v1\", render_mode=\"human\")\n",
    "\n",
    "# Reset to start a new episode\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# Run for 5100 steps with random actions\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()           # sample a random action (0 or 1)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we introduce a solution here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Taxi\n",
    "This task was introduced  to illustrate some issues in hierarchical reinforcement learning. <br>\n",
    "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. <br>\n",
    "You receive +20 points for a successful drop-off, and lose 1 point for every timestep it takes. <br>\n",
    "There is also a 10 point penalty for illegal pick-up and drop-off actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxi: random action demo using Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment (requires gymnasium[toy-text])\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "\n",
    "# Reset to start a new episode\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# Run for 1000 steps with random actions\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()            # pick a random discrete action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "def get_angles(obs):\n",
    "    c1, s1, c2, s2, th1dot, th2dot = obs\n",
    "    theta1, theta2 = np.arctan2(s1, c1), np.arctan2(s2, c2)\n",
    "    return theta1, theta2, th1dot, th2dot\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()            \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "ep_reward = 0.0\n",
    "for t in range(500):\n",
    "    theta1, theta2, th1dot, th2dot = get_angles(obs)\n",
    "\n",
    "    energy = 0.5*(th1dot**2 + th2dot**2) - np.cos(theta1) - np.cos(theta1 + theta2)\n",
    "    \n",
    "    action = 1 if energy < 1.0 else 0\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    ep_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Episode reward (energy heuristic): {ep_reward:.1f}\")\n",
    "        obs, info, ep_reward = env.reset(), env.reset()[1], 0.0\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflection Questions\n",
    "# 1. I noticed that, mainly, the agent's actions (on average) became more consistent over time. It would take a lot longer to train one to become \"better\" over time than we are given here, however, this was enough for them to become more consistent.\n",
    "# 2. This one would change specificaly depending on the model (duh...), but I noticed the main change in the Bipedal model. As I stated in it, I ended up messing with the Alpha value. I noticed that when I increased the value, the model got smoother. However, smoother doesn't always mean better, so after running it at a few different values, I found .85 to be the best.\n",
    "# 3. Simplest, it is in my opinion a very fun way of using reinforcement learning. You essentialy let it free in an environment and watch it learn the best ways to use each\n",
    "#    it's given actions. This makes it interesting to watch and my favorite part of AI. Which is ironically also why this part took so long - I wanted to absolutely ensure I went\n",
    "#    through it so meticulously so that I know what I am doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement Kernel",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
